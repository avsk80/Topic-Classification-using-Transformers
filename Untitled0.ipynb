{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"11T2flFxfZ5cLhg1pi1ruSVqzhxtoYjNf","authorship_tag":"ABX9TyPucWBwSJMangsw7goBlD+c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yVjDrSyevHpJ","executionInfo":{"status":"ok","timestamp":1682902134435,"user_tz":300,"elapsed":11431,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"05192324-7199-4453-df2d-253714fc21ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import transformers\n","import torch\n","from torch.utils.data import Dataset, DataLoader"],"metadata":{"id":"WsGdc5IJCMqT","executionInfo":{"status":"ok","timestamp":1682902144559,"user_tz":300,"elapsed":10126,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(\"/content/drive/MyDrive/pro/8701/combined_14510_xlnet.csv\")\n","title_only = df[(df['title_polyglot_detect'] == 'en') & (df['title_lang_detect'] == 'en') & (df['title_langid_detect'] == 'en') & (df['title_xl_detect'] == 'en')][['question_title', 'class_index']]\n","\n","from sklearn.model_selection import train_test_split\n","X_tmp, X_test, y_tmp, y_test = train_test_split(title_only['question_title'], title_only['class_index'], stratify=title_only['class_index'], test_size=0.10, random_state=42)\n","X_tmp.shape, X_test.shape, y_tmp.shape, y_test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FdeLJk_XCW12","executionInfo":{"status":"ok","timestamp":1682902146453,"user_tz":300,"elapsed":1915,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"70facd91-cb9f-4cf7-bebd-39ea2864f6a9"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14737,), (1638,), (14737,), (1638,))"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["tmp = pd.DataFrame({\n","    \"question_title\": X_tmp,\n","    \"class_index\": y_tmp\n","}\n",")\n","# tmp.class_index.value_counts()\n","X_train, X_valid, y_train, y_valid = train_test_split(tmp['question_title'], tmp['class_index'], stratify=tmp['class_index'], test_size=0.10, random_state=42)\n","X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ooIqotjikCa","executionInfo":{"status":"ok","timestamp":1682902146455,"user_tz":300,"elapsed":21,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"733ead67-d788-4467-86fa-157322691b4c"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((13263,), (1474,), (13263,), (1474,))"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["(X_train.shape, X_test.shape, X_valid.shape), (y_train.shape, y_test.shape, y_valid.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s1ISzMLBjRko","executionInfo":{"status":"ok","timestamp":1682902146457,"user_tz":300,"elapsed":16,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"8c626887-1506-4d74-8e04-0e15ecb63573"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(((13263,), (1638,), (1474,)), ((13263,), (1638,), (1474,)))"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["class YahooDataset(Dataset):\n"," \n","  def __init__(self, x, y):\n","    self.x=x.values\n","    self.y=y.values\n","  \n","  def __len__(self):\n","    return len(self.y)\n","   \n","  def __getitem__(self,idx):\n","    return self.x[idx],self.y[idx]"],"metadata":{"id":"NXhEyXwvC4HX","executionInfo":{"status":"ok","timestamp":1682790520616,"user_tz":300,"elapsed":204,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from transformers import BertModel, BertTokenizer\n","bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","bert_model = BertModel.from_pretrained('bert-base-uncased')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OtN7AurftVuz","executionInfo":{"status":"ok","timestamp":1682902152086,"user_tz":300,"elapsed":5305,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"e49e6dab-a958-4b10-e0d9-63958a050f7e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["# bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize(\"This is superb!\"))\n","\n","# The senetence to be encoded\n","sent = \"Let's learn deep learning!\"\n","\n","# Encode the sentence\n","encoded = bert_tokenizer.encode_plus(\n","    text=sent,  # the sentence to be encoded\n","    add_special_tokens=True,  # Add [CLS] and [SEP]\n","    max_length = 64,  # maximum length of a sentence\n","    pad_to_max_length=True,  # Add [PAD]s\n","    return_attention_mask = True,  # Generate the attention mask\n","    return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",")\n","\n","# Get the input IDs and attention mask in tensor format\n","input_ids = encoded['input_ids']\n","attn_mask = encoded['attention_mask']\n","\n","input_ids, attn_mask"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Afc-n24vlm2m","executionInfo":{"status":"ok","timestamp":1682899279906,"user_tz":300,"elapsed":231,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"280b1895-9c3c-4877-d294-5f40125206e9"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["(tensor([[ 101, 2292, 1005, 1055, 4553, 2784, 4083,  999,  102,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0]]),\n"," tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["bert_tokenizer.convert_ids_to_tokens(ids=input_ids[0], skip_special_tokens=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xqgj7Pc5lnnr","executionInfo":{"status":"ok","timestamp":1682899550889,"user_tz":300,"elapsed":3,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"9d9e435e-6fdd-4aa5-9873-f099b19d2801"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['let', \"'\", 's', 'learn', 'deep', 'learning', '!']"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["def encode(x, y, tokenizer):\n","    input_ids = []\n","    attention_mask = []\n","\n","    labels = np.unique(y)\n","    id2label = {idx:label for idx, label in enumerate(labels)}\n","    label2id = {label:idx for idx, label in enumerate(labels)}\n","    \n","    for text in x:\n","        tokenized_text = tokenizer.encode_plus(text,\n","                                            max_length=512,\n","                                            add_special_tokens=True,\n","                                            pad_to_max_length=True,\n","                                            return_attention_mask=True)\n","        input_ids.append(tokenized_text['input_ids'])\n","        attention_mask.append(tokenized_text['attention_mask'])\n","    y = [ label2id[label] for label in y]\n","    return torch.tensor(input_ids, dtype=torch.long), torch.tensor(attention_mask, dtype=torch.long), torch.tensor(y, dtype=torch.long)"],"metadata":{"id":"JmLsMAjesrhl","executionInfo":{"status":"ok","timestamp":1682902152087,"user_tz":300,"elapsed":12,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def get_batches(x, y, tokenizer, batch_size=8):\n","    input_ids, attention_mask, y = encode(x, y, tokenizer)\n","    tensor_dataset = torch.utils.data.TensorDataset(input_ids, attention_mask, y)\n","    # tensor_randomsampler = torch.utils.data.RandomSampler(tensor_dataset)\n","    tensor_dataloader = torch.utils.data.DataLoader(tensor_dataset, batch_size=batch_size) # sampler=tensor_randomsampler,\n","    return tensor_dataloader"],"metadata":{"id":"bocOmk-msxdw","executionInfo":{"status":"ok","timestamp":1682902152089,"user_tz":300,"elapsed":10,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# input_ids, attention_mask, y = encode(list(X_train), list(y_train), tokenizer=bert_tokenizer)"],"metadata":{"id":"OciBUPhN6Zzr","executionInfo":{"status":"ok","timestamp":1682753376455,"user_tz":300,"elapsed":12983,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# yd_trn = YahooDataset(x=X_train, y=y_train)\n","# yd_trn_dl = DataLoader(yd_trn)\n","\n","# type(yd_trn_dl), type(yd_trn)\n","\n","train_dl = get_batches(list(X_train), list(y_train), tokenizer=bert_tokenizer)\n","valid_dl = get_batches(list(X_valid), list(y_valid), tokenizer=bert_tokenizer)\n","test_dl = get_batches(list(X_test), list(y_test), tokenizer=bert_tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4I161TsVeXBu","executionInfo":{"status":"ok","timestamp":1682902175550,"user_tz":300,"elapsed":22008,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"9cf75939-b5ca-4618-f315-ca156bd485bf"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["def train_model(batch, model, optimizer, scheduler, epochs, device):\n","    model.train()  # Set the mode to training\n","    for e in range(epochs):\n","        for i, batch_tuple in enumerate(batch):\n","            batch_tuple = (t.to(device) for t in batch_tuple)\n","            input_ids, attention_mask, labels = batch_tuple\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss, logits, hidden_states_output, attention_mask_output = outputs\n","            if i % 100 == 0:\n","                print(\"loss - {0}, iteration - {1}/{2}\".format(loss, e + 1, i))\n","            model.zero_grad()\n","            optimizer.zero_grad()\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), parameters['max_grad_norm'])\n","            optimizer.step()\n","            scheduler.step()\n","\n","def evaluate(batch, model, device):\n","    input_ids, predictions, true_labels, attentions = [], [], [], []\n","    model.eval()\n","    for i, batch_cpu in enumerate(batch):\n","        batch_gpu = (t.to(device) for t in batch_cpu)\n","        input_ids_gpu, attention_mask, labels = batch_gpu\n","        with torch.no_grad():\n","            loss, logits, hidden_states_output, attention_mask_output = model(input_ids=input_ids_gpu, attention_mask=attention_mask, labels=labels)\n","            logits =  logits.cpu()\n","            prediction = torch.argmax(logits, dim=1).tolist()\n","            true_label = labels.cpu().tolist()\n","            input_ids_cpu = input_ids_gpu.cpu().tolist()\n","            attention_last_layer = attention_mask_output[-1].cpu() # selection the last attention layer\n","            attention_softmax = attention_last_layer[:,-1, 0].tolist()  # selection the last head attention of CLS token\n","            input_ids += input_ids_cpu\n","            predictions += prediction\n","            true_labels += true_label\n","            attentions += attention_softmax\n","    return input_ids, predictions, true_labels, attentions"],"metadata":{"id":"z6KsoV2G7ldQ","executionInfo":{"status":"ok","timestamp":1682901899230,"user_tz":300,"elapsed":5,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["epochs=2\n","parameters = {\n","    'learning_rate': 5e-3,\n","    'num_warmup_steps': 1000,\n","    'num_training_steps': len(train_dl) * epochs,\n","    'max_grad_norm': 1\n","}\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","bert_model = BertModel.from_pretrained('bert-base-uncased', num_labels=4, output_hidden_states=True, output_attentions=True)\n","bert_model.to(device)\n","optimizer = transformers.AdamW(bert_model.parameters(), lr=parameters['learning_rate'], correct_bias=False)\n","scheduler = transformers.get_linear_schedule_with_warmup(optimizer,\n","                                                         num_warmup_steps=parameters['num_warmup_steps'],\n","                                                         num_training_steps=parameters['num_training_steps'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yjo5Kmkg7lcr","executionInfo":{"status":"ok","timestamp":1682901913264,"user_tz":300,"elapsed":8076,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"5fee47b1-c06c-49d1-810e-c63303573571"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["train_model(train_dl, bert_model, optimizer, scheduler, epochs, device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":432},"id":"YNW3lDO67lZp","executionInfo":{"status":"error","timestamp":1682795621562,"user_tz":300,"elapsed":508,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"43b636f6-8214-4b25-cf72-5aaa6719833c"},"execution_count":35,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-93903d88f437>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-11-15513366a872>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(batch, model, optimizer, scheduler, epochs, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mbatch_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: BertModel.forward() got an unexpected keyword argument 'labels'"]}]},{"cell_type":"code","source":["t = [ i for i in train_dl]\n","t"],"metadata":{"id":"oMSggRIU7lZE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(t)\n","# t[0][0][0]\n","len(t[0][1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gQVzW1vo7gmC","executionInfo":{"status":"ok","timestamp":1682795654663,"user_tz":300,"elapsed":240,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"8cc9999f-8b22-4138-a1ba-dd09b165c350"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["op = dict()\n","for i, batch_tuple in enumerate(train_dl):\n","    batch_tuple = (t for t in batch_tuple)\n","    # print(len([i for i in batch_tuple][2]))\n","    input_ids, attention_mask, label = batch_tuple\n","    op = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n","    if i == 2:\n","      break"],"metadata":{"id":"uZXgjAaTJvkM","executionInfo":{"status":"ok","timestamp":1682902273009,"user_tz":300,"elapsed":75015,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["op = bert_model()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":287},"id":"JhYppq8CLb07","executionInfo":{"status":"error","timestamp":1682902441895,"user_tz":300,"elapsed":267,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"1ad76d94-8e67-493e-d2b5-9f0ba8a20b7d"},"execution_count":24,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-a4516891131a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0minner_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'logits'"]}]},{"cell_type":"code","source":["len(op[0][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tjmmhzk0y449","executionInfo":{"status":"ok","timestamp":1682902753769,"user_tz":300,"elapsed":8,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"01da2d93-55f4-4010-ab9e-6b223490fc5e"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["512"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":[],"metadata":{"id":"7sqJoEGW13jo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"G2Y9i_Ks13fS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"E0XGnvjj13eV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gwSI382W13bZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gzip\n","import shutil\n","import time\n","import pandas as pd\n","import requests\n","import torch\n","import torch.nn.functional as F\n","import torchtext\n","import transformers\n","from transformers import DistilBertTokenizerFast\n","from transformers import DistilBertForSequenceClassification\n","\n","from transformers import BertTokenizer\n","from transformers import BertForSequenceClassification"],"metadata":{"id":"WTIyGCBT13af","executionInfo":{"status":"ok","timestamp":1682921582750,"user_tz":300,"elapsed":276,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}}},"execution_count":214,"outputs":[]},{"cell_type":"code","source":["torch.backends.cudnn.deterministic = True\n","RANDOM_SEED = 123\n","torch.manual_seed(RANDOM_SEED)\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","NUM_EPOCHS = 3"],"metadata":{"id":"1hco69aq13XK","executionInfo":{"status":"ok","timestamp":1682903179529,"user_tz":300,"elapsed":254,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Qu_WEc8u13V9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(\"/content/drive/MyDrive/pro/8701/combined_14510_xlnet.csv\")\n","title_only = df[(df['title_polyglot_detect'] == 'en') & (df['title_lang_detect'] == 'en') & (df['title_langid_detect'] == 'en') & (df['title_xl_detect'] == 'en')][['question_title', 'class_index']]\n","\n","from sklearn.model_selection import train_test_split\n","X_tmp, X_test, y_tmp, y_test = train_test_split(title_only['question_title'], title_only['class_index'], stratify=title_only['class_index'], test_size=0.10, random_state=42)\n","X_tmp.shape, X_test.shape, y_tmp.shape, y_test.shape, type(X_tmp)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682908370740,"user_tz":300,"elapsed":714,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"99bc6b20-eef7-458e-8914-80392ed5bdce","id":"5g_FGP0B2U5u"},"execution_count":138,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((14737,), (1638,), (14737,), (1638,), pandas.core.series.Series)"]},"metadata":{},"execution_count":138}]},{"cell_type":"code","source":["tmp = pd.DataFrame({\n","    \"question_title\": X_tmp,\n","    \"class_index\": y_tmp\n","}\n",")\n","# tmp.class_index.value_counts()\n","X_train, X_valid, y_train, y_valid = train_test_split(tmp['question_title'], tmp['class_index'], stratify=tmp['class_index'], test_size=0.10, random_state=42)\n","X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682908372676,"user_tz":300,"elapsed":295,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"7b989e5f-58ce-477a-8a36-d4df8d905e10","id":"_dgL9Qtz2U5w"},"execution_count":139,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((13263,), (1474,), (13263,), (1474,))"]},"metadata":{},"execution_count":139}]},{"cell_type":"code","source":["def tokenize(list_of_sentences, tokenizer):\n","  for i in list_of_sentences:\n","    # print(list_of_sentences[i])\n","    encoded = tokenizer.encode_plus(\n","        text=i,  # the sentence to be encoded\n","        add_special_tokens=True,  # Add [CLS] and [SEP]\n","        max_length = 512,  # maximum length of a sentence\n","        pad_to_max_length=True,  # Add [PAD]s\n","        return_attention_mask = True,  # Generate the attention mask\n","        return_tensors = 'pt',  # ask the function to return PyTorch tensors\n","    )\n","  return encoded\n","\n","tokenizer = DistilBertTokenizerFast.from_pretrained(\n","     'distilbert-base-uncased', num_labels = 4\n",")\n","\n","train_encodings = tokenize(list(X_train), tokenizer)\n","valid_encodings = tokenize(list(X_valid), tokenizer)\n","test_encodings = tokenize(list(X_test), tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EBT0Vfbj0YYL","executionInfo":{"status":"ok","timestamp":1682920781119,"user_tz":300,"elapsed":15990,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"67425aa3-a629-4ee8-bcef-c1be882b797e"},"execution_count":199,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["train_encodings[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rjpM3D5SHyv4","executionInfo":{"status":"ok","timestamp":1682920784470,"user_tz":300,"elapsed":266,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"202a3dd2-32c3-4415-b093-f13cd564d9f4"},"execution_count":200,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"]},"metadata":{},"execution_count":200}]},{"cell_type":"code","source":["# tokenizer.convert_ids_to_tokens(train_encodings[0].ids, skip_special_tokens=True)\n","# train_encodings.items()"],"metadata":{"id":"aZfZqiVB3dde","executionInfo":{"status":"ok","timestamp":1682920788521,"user_tz":300,"elapsed":272,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}}},"execution_count":201,"outputs":[]},{"cell_type":"code","source":["class YahooDataset(torch.utils.data.Dataset):\n","  def __init__(self, encodings, labels):\n","    self.encodings = encodings\n","    label_to_id = {\n","        1: 0,\n","        5: 1,\n","        6: 2,\n","        10: 3\n","    }\n","    self.labels = [ label_to_id[i] for i in labels]\n","  \n","  def __getitem__(self, idx):\n","    item = {key: torch.tensor(val[idx]) \n","            for key, val in self.encodings.items()}\n","    item['labels'] = torch.tensor(self.labels[idx])\n","    return item\n","\n","  def __len__(self):\n","    return len(self.labels)\n","\n","train_dataset = YahooDataset(train_encodings, list(y_train))\n","valid_dataset = YahooDataset(valid_encodings, list(y_valid))\n","test_dataset = YahooDataset(test_encodings, list(y_test))\n","train_loader = DataLoader(train_dataset, batch_size=8) \n","valid_loader = DataLoader(valid_dataset, batch_size=8) \n","test_loader = DataLoader(test_dataset, batch_size=8)"],"metadata":{"id":"ePwEMOdl00IL","executionInfo":{"status":"ok","timestamp":1682920790237,"user_tz":300,"elapsed":421,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}}},"execution_count":202,"outputs":[]},{"cell_type":"code","source":["# train_encodings.keys()\n","# { k:v  for k, v in train_encodings.items()}\n","# train_dataset[0]\n","train_loader\n","# train_dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7t7jymYP6-od","executionInfo":{"status":"ok","timestamp":1682920796443,"user_tz":300,"elapsed":357,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"128ff583-9f44-4ef3-aa95-850df7c9e882"},"execution_count":203,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.utils.data.dataloader.DataLoader at 0x7f91b9986890>"]},"metadata":{},"execution_count":203}]},{"cell_type":"code","source":["model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', output_attentions=True)\n","model.to(DEVICE)\n","model.train()\n","optim = torch.optim.Adam(model.parameters(), lr=5e-5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ckazRowA9MbC","executionInfo":{"status":"ok","timestamp":1682920800077,"user_tz":300,"elapsed":1139,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"92c2f30f-2aa2-4c24-a2e4-0a51e87d7b2f"},"execution_count":204,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["def compute_accuracy(model, data_loader, device):\n","     with torch.no_grad():\n","         correct_pred, num_examples = 0, 0\n","         for batch_idx, batch in enumerate(data_loader):\n","             ### Prepare data\n","             input_ids = batch['input_ids'].to(device)\n","             attention_mask = batch['attention_mask'].to(device)\n","             labels = batch['labels'].to(device)\n","    \n","             outputs = model(input_ids, attention_mask=attention_mask)\n","             logits = outputs['logits']\n","             predicted_labels = torch.argmax(logits, 1)\n","             num_examples += labels.size(0)\n","             correct_pred += (predicted_labels == labels).sum()\n","     return correct_pred.float()/num_examples * 100"],"metadata":{"id":"FG6--DKY-ueT","executionInfo":{"status":"ok","timestamp":1682920806531,"user_tz":300,"elapsed":265,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}}},"execution_count":205,"outputs":[]},{"cell_type":"code","source":["start_time = time.time()\n","for epoch in range(NUM_EPOCHS):\n","  model.train()\n","   \n","  for batch_idx, batch in enumerate(train_loader):\n","    ### Prepare data\n","    input_ids = batch['input_ids'].to(DEVICE)\n","    attention_mask = batch['attention_mask'].to(DEVICE)\n","    labels = batch['labels'].to(DEVICE)\n","    ### Forward pass\n","    outputs = model(input_ids, \n","                    attention_mask=attention_mask,\n","                    labels=labels)\n","    loss, logits = outputs['loss'], outputs['logits']\n","\n","    ### Backward pass\n","    optim.zero_grad()\n","    loss.backward()\n","    optim.step()\n","\n","    ### Logging\n","    if not batch_idx % 250:\n","      print(f'Epoch: {epoch+1:04d}/{NUM_EPOCHS:04d}' \n","                f' | Batch' \n","                f'{batch_idx:04d}/'\n","                f'{len(train_loader):04d} | '\n","                f'Loss: {loss:.4f}')\n","  \n","  model.eval()\n","  with torch.set_grad_enabled(False):\n","    print(f'Training accuracy: '\n","            f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n","            f'\\nValid accuracy: '\n","            f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%')\n","    \n","  print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n"," \n","print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n","print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":435},"id":"iDh65Wev-udQ","executionInfo":{"status":"error","timestamp":1682920808691,"user_tz":300,"elapsed":814,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"77faccec-9d5a-42f7-cb2b-0015b52ff2a8"},"execution_count":206,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-202-89b2b7e1f656>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[idx])\n"]},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-206-6dc5da475489>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m### Prepare data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-202-89b2b7e1f656>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     item = {key: torch.tensor(val[idx]) \n\u001b[0m\u001b[1;32m     14\u001b[0m             for key, val in self.encodings.items()}\n\u001b[1;32m     15\u001b[0m     \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-202-89b2b7e1f656>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     item = {key: torch.tensor(val[idx]) \n\u001b[0m\u001b[1;32m     14\u001b[0m             for key, val in self.encodings.items()}\n\u001b[1;32m     15\u001b[0m     \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"]}]},{"cell_type":"code","source":["for batch_idx, batch in enumerate(train_loader):\n","  print(batch_idx)\n","  print(batch)\n","  if batch_idx == 2:\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":435},"id":"-5kSxH45-uZY","executionInfo":{"status":"error","timestamp":1682920813340,"user_tz":300,"elapsed":242,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"b43ba16c-0479-48ad-8a30-6fa518131e54"},"execution_count":207,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-202-89b2b7e1f656>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[idx])\n"]},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-207-2b5baf8d3697>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-202-89b2b7e1f656>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     item = {key: torch.tensor(val[idx]) \n\u001b[0m\u001b[1;32m     14\u001b[0m             for key, val in self.encodings.items()}\n\u001b[1;32m     15\u001b[0m     \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-202-89b2b7e1f656>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     item = {key: torch.tensor(val[idx]) \n\u001b[0m\u001b[1;32m     14\u001b[0m             for key, val in self.encodings.items()}\n\u001b[1;32m     15\u001b[0m     \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"]}]},{"cell_type":"code","source":["for i in train_loader:\n","  print(i)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"47K7C2MS-uYO","executionInfo":{"status":"error","timestamp":1682920817551,"user_tz":300,"elapsed":253,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"eeb73c41-46f9-486c-cebc-31df732db375"},"execution_count":208,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-202-89b2b7e1f656>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[idx])\n"]},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-208-07f1b598aa84>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-202-89b2b7e1f656>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     item = {key: torch.tensor(val[idx]) \n\u001b[0m\u001b[1;32m     14\u001b[0m             for key, val in self.encodings.items()}\n\u001b[1;32m     15\u001b[0m     \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-202-89b2b7e1f656>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     item = {key: torch.tensor(val[idx]) \n\u001b[0m\u001b[1;32m     14\u001b[0m             for key, val in self.encodings.items()}\n\u001b[1;32m     15\u001b[0m     \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"]}]},{"cell_type":"code","source":[],"metadata":{"id":"iIy4HKOSBqwM","executionInfo":{"status":"ok","timestamp":1682920833670,"user_tz":300,"elapsed":258,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}}},"execution_count":208,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bRU_jc-cK2T4","executionInfo":{"status":"ok","timestamp":1682920834485,"user_tz":300,"elapsed":2,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}}},"execution_count":208,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","class TextClassificationDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len):\n","        self.texts = texts\n","        label_to_id = {\n","        1: 0,\n","        5: 1,\n","        6: 2,\n","        10: 3\n","        }\n","        self.labels = [ label_to_id[i] for i in labels]\n","        # self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        \n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        label = self.labels[idx]\n","\n","        # Tokenize text and truncate to max_len\n","        inputs = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            truncation=True,\n","            padding='max_length',\n","            return_attention_mask=True,\n","            return_token_type_ids=False,\n","            return_tensors='pt'\n","        )\n","\n","        return inputs['input_ids'], inputs['attention_mask'], label\n","\n","# Example usage:\n","# texts = [\"This is a positive text\", \"This is a negative text\"]\n","# labels = [1, 0] # 1 for positive, 0 for negative\n","texts = list(X_train)[:7]\n","labels = list(y_train)[:7]\n","tokenizer = tokenizer # Initialize your tokenizer\n","max_len = 32\n","dataset = TextClassificationDataset(texts, labels, tokenizer, max_len)\n","train_dataloader = DataLoader(dataset, batch_size=3)\n","valid_dataloader = DataLoader(dataset, batch_size=3)\n","\n","# Iterate through batches\n","for batch in train_dataloader:\n","    input_ids, attention_mask, labels = batch\n","    print(input_ids)\n","    print(attention_mask)\n","    print(labels)\n","\n","len(train_dataloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vCaFcChsK2TC","executionInfo":{"status":"ok","timestamp":1682920840589,"user_tz":300,"elapsed":264,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"dc165178-cc8f-4db8-8475-42587260fe99"},"execution_count":209,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[  101,  2040,  1005,  1055,  6069,  2663,  1996,  6156,  2452,  1029,\n","            102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0]],\n","\n","        [[  101, 18106, 14924, 26340,  1029,  2064, 10334,  2507,  2033,  2204,\n","           2609,  2005,  7760, 18471, 14924, 26340,  1029,   102,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0]],\n","\n","        [[  101,  2054,  2064,  8135,  2175,  2000,  3109,  2005,  1029,   102,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0]]])\n","tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0]],\n","\n","        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0]],\n","\n","        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0]]])\n","tensor([2, 1, 0])\n","tensor([[[ 101, 2054, 2112, 1997, 6206, 2987, 1005, 1056, 2577, 1059, 5747,\n","          1998, 1996, 4001, 3305, 1029,  102,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0]],\n","\n","        [[ 101, 2339, 2064, 1005, 1056, 1045, 2424, 1037, 4037, 2007, 2023,\n","          3933, 1029,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0]],\n","\n","        [[ 101, 2054, 4148, 5732, 2000, 9679, 6466, 1029,  102,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0]]])\n","tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0]],\n","\n","        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0]],\n","\n","        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0]]])\n","tensor([3, 2, 2])\n","tensor([[[ 101, 2339, 2024, 7588, 2200, 3928, 1029,  102,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0]]])\n","tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0]]])\n","tensor([1])\n"]},{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":209}]},{"cell_type":"code","source":["from transformers.models.distilbert.modeling_distilbert import DistilBertModel\n","model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n","\n","for i, b in enumerate(train_dataloader):\n","  input_ids, attention_mask, label = b\n","  print(input_ids)\n","  print(attention_mask)\n","  print(label)for i, b in enumerate(train_dataloader):\n","  input_ids, attention_mask, label = b\n","  print(input_ids)\n","  print(attention_mask)\n","  print(label)\n","  op = model(input_ids = input_ids, attention_mask=attention_mask)\n","  op = model(input_ids = input_ids, attention_mask=attention_mask)"],"metadata":{"id":"D06dn1vDlVfd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model()"],"metadata":{"id":"A3-hK894t-z-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define optimizer and loss function\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.05)\n","loss_fn = torch.nn.CrossEntropyLoss()\n","# device = 'cpu'\n","model.train()\n","for epoch in range(epochs):\n","    for batch in train_dataloader:\n","        optimizer.zero_grad()\n","        # Get inputs and labels\n","        input_ids, attention_mask, labels = batch\n","        input_ids = input_ids.to(DEVICE)\n","        attention_mask = attention_mask.to(DEVICE)\n","        labels = labels.to(DEVICE)\n","    \n","        # Forward pass\n","        outputs = model(input_ids= input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","\n","        # Compute loss and backpropagate\n","        loss = loss_fn(logits, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Print training loss for this epoch\n","    print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"id":"D2XbJdw-kr4T","executionInfo":{"status":"error","timestamp":1682920854935,"user_tz":300,"elapsed":266,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"7a6f657f-68dd-416c-a190-bb29e44833dd"},"execution_count":211,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-211-be9b16365a9f>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         return self.transformer(\n\u001b[0m\u001b[1;32m    584\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \"\"\"\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Self-Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         sa_output = self.attention(\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    196\u001b[0m             seq_length, dim) Contextualized layer. Optional: only if `output_attentions=True`\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0mk_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;31m# assert dim == self.dim, f'Dimensions do not match: {dim} input vs {self.dim} configured'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"]}]},{"cell_type":"code","source":["# Define hyperparameters\n","epochs = 1\n","batch_size = 8\n","learning_rate = 5e-2\n","\n","start_time = time.time()\n","for epoch in range(NUM_EPOCHS):\n","  model.train()\n","  \n","  for batch_idx, batch in enumerate(train_dataloader):\n","    ### Prepare data\n","    input_ids, attention_mask, labels = batch\n","    input_ids = input_ids.to(DEVICE)\n","    attention_mask = attention_mask.to(DEVICE)\n","    labels = labels.to(DEVICE)\n","    print(input_ids.shape)\n","    print(attention_mask.shape)\n","    print(labels.shape)\n","    ### Forward pass\n","    outputs = \\\n","    model(input_ids, \n","                    attention_mask=attention_mask)\n","    loss, logits = outputs['loss'], outputs['logits']\n","\n","    ### Backward pass\n","    optim.zero_grad()\n","    loss.backward()\n","    optim.step()\n","\n","    ### Logging\n","    if not batch_idx % 250:\n","      print(f'Epoch: {epoch+1:04d}/{NUM_EPOCHS:04d}' \n","                f' | Batch' \n","                f'{batch_idx:04d}/'\n","                f'{len(train_loader):04d} | '\n","                f'Loss: {loss:.4f}')\n","  \n","  model.eval()\n","  with torch.set_grad_enabled(False):\n","    print(f'Training accuracy: '\n","            f'{compute_accuracy(model, train_dataloader, DEVICE):.2f}%'\n","            f'\\nValid accuracy: '\n","            f'{compute_accuracy(model, valid_dataloader, DEVICE):.2f}%')\n","    \n","  print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n","\n","print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n","print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449},"id":"nRym7z96K2PO","executionInfo":{"status":"error","timestamp":1682920888702,"user_tz":300,"elapsed":274,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"d9e7462c-b1eb-40fb-c24a-09d3af15fa15"},"execution_count":213,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 1, 32])\n","torch.Size([3, 1, 32])\n","torch.Size([3])\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-213-b59dffe4cd6b>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m### Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     model(input_ids, \n\u001b[0m\u001b[1;32m     22\u001b[0m                     attention_mask=attention_mask)\n\u001b[1;32m     23\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         return self.transformer(\n\u001b[0m\u001b[1;32m    584\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \"\"\"\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Self-Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         sa_output = self.attention(\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    196\u001b[0m             seq_length, dim) Contextualized layer. Optional: only if `output_attentions=True`\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0mk_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;31m# assert dim == self.dim, f'Dimensions do not match: {dim} input vs {self.dim} configured'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"]}]},{"cell_type":"code","source":[],"metadata":{"id":"vgJFJoJ3ke_B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"J1WQyaTs8aWY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize(list_of_sentences, tokenizer):\n","  for i in list_of_sentences:\n","    # print(list_of_sentences[i])\n","    encoded = tokenizer.encode_plus(\n","        text=i,  # the sentence to be encoded\n","        add_special_tokens=True,  # Add [CLS] and [SEP]\n","        max_length = 512,  # maximum length of a sentence\n","        pad_to_max_length=True,  # Add [PAD]s\n","        return_attention_mask = True,  # Generate the attention mask\n","        return_tensors = 'pt',  # ask the function to return PyTorch tensors\n","    )\n","  return encoded\n","\n","tokenizer = BertTokenizer.from_pretrained(\n","     'bert-base-uncased', num_labels = 4\n",")\n","\n","train_encodings = tokenize(list(X_train), tokenizer)\n","valid_encodings = tokenize(list(X_valid), tokenizer)\n","test_encodings = tokenize(list(X_test), tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VJaCN36Y8aVa","executionInfo":{"status":"ok","timestamp":1682921715077,"user_tz":300,"elapsed":21398,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"da331d86-d63c-4007-994f-5722cd7470e5"},"execution_count":215,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","class TextClassificationDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len):\n","        self.texts = texts\n","        label_to_id = {\n","        1: 0,\n","        5: 1,\n","        6: 2,\n","        10: 3\n","        }\n","        self.labels = [ label_to_id[i] for i in labels]\n","        # self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        \n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        label = self.labels[idx]\n","\n","        # Tokenize text and truncate to max_len\n","        inputs = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            truncation=True,\n","            padding='max_length',\n","            return_attention_mask=True,\n","            return_token_type_ids=False,\n","            return_tensors='pt'\n","        )\n","\n","        return inputs['input_ids'], inputs['attention_mask'], label\n","\n","# Example usage:\n","# texts = [\"This is a positive text\", \"This is a negative text\"]\n","# labels = [1, 0] # 1 for positive, 0 for negative\n","texts = list(X_train)[:7]\n","labels = list(y_train)[:7]\n","tokenizer = BertTokenizer.from_pretrained(\n","     'bert-base-uncased', num_labels = 4\n",") # Initialize your tokenizer\n","max_len = 32\n","dataset = TextClassificationDataset(texts, labels, tokenizer, max_len)\n","train_dataloader = DataLoader(dataset, batch_size=3)\n","valid_dataloader = DataLoader(dataset, batch_size=3)\n","\n","# Iterate through batches\n","for batch in train_dataloader:\n","    input_ids, attention_mask, labels = batch\n","    print(input_ids)\n","    print(attention_mask)\n","    print(labels)\n","\n","len(train_dataloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C0zW4RH-8yCN","executionInfo":{"status":"ok","timestamp":1682922054617,"user_tz":300,"elapsed":869,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"eab25c51-d04e-4a72-e797-a3651ba05210"},"execution_count":218,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[  101,  2040,  1005,  1055,  6069,  2663,  1996,  6156,  2452,  1029,\n","            102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0]],\n","\n","        [[  101, 18106, 14924, 26340,  1029,  2064, 10334,  2507,  2033,  2204,\n","           2609,  2005,  7760, 18471, 14924, 26340,  1029,   102,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0]],\n","\n","        [[  101,  2054,  2064,  8135,  2175,  2000,  3109,  2005,  1029,   102,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0]]])\n","tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0]],\n","\n","        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0]],\n","\n","        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0]]])\n","tensor([2, 1, 0])\n","tensor([[[ 101, 2054, 2112, 1997, 6206, 2987, 1005, 1056, 2577, 1059, 5747,\n","          1998, 1996, 4001, 3305, 1029,  102,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0]],\n","\n","        [[ 101, 2339, 2064, 1005, 1056, 1045, 2424, 1037, 4037, 2007, 2023,\n","          3933, 1029,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0]],\n","\n","        [[ 101, 2054, 4148, 5732, 2000, 9679, 6466, 1029,  102,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0]]])\n","tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0]],\n","\n","        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0]],\n","\n","        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0]]])\n","tensor([3, 2, 2])\n","tensor([[[ 101, 2339, 2024, 7588, 2200, 3928, 1029,  102,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0]]])\n","tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0]]])\n","tensor([1])\n"]},{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":218}]},{"cell_type":"code","source":["from torch import nn\n","class BertClassifier(nn.Module):\n","\n","    def __init__(self): #, dropout=0.5\n","\n","        super(BertClassifier, self).__init__()\n","\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","        # self.dropout = nn.Dropout(dropout)\n","        # self.linear = nn.Linear(768, 4)\n","        # self.relu = nn.ReLU()\n","\n","    def forward(self, input_id, mask):\n","\n","        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n","    #     dropout_output = self.dropout(pooled_output)\n","    #     linear_output = self.linear(dropout_output)\n","    #     final_layer = self.relu(linear_output)\n","\n","        return pooled_output"],"metadata":{"id":"5BkE0sBMAypB","executionInfo":{"status":"ok","timestamp":1682922802536,"user_tz":300,"elapsed":511,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}}},"execution_count":225,"outputs":[]},{"cell_type":"code","source":["model = BertClassifier()\n","\n","for i, b in enumerate(train_dataloader):\n","  b_input_ids, b_attention_mask, b_label = b\n","  print(input_ids)\n","  print(attention_mask)\n","  print(label)\n","  op = model(input_id = b_input_ids, mask=b_attention_mask)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":556},"id":"zf93iC4t89Yf","executionInfo":{"status":"error","timestamp":1682922895311,"user_tz":300,"elapsed":2842,"user":{"displayName":"Venkata Sai Krishna Abbaraju","userId":"03956108207244612420"}},"outputId":"09552d42-1e27-4f8d-b5eb-7586c9b1e2d9"},"execution_count":228,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["tensor([[[ 101, 2339, 2024, 7588, 2200, 3928, 1029,  102,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","             0,    0,    0,    0,    0,    0,    0,    0,    0,    0]]])\n","tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","          0, 0, 0, 0, 0, 0, 0, 0, 0]]])\n","tensor([1])\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-228-c7bbc7fa2ca0>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-225-201e529bc995>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_id, mask)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;31m#     dropout_output = self.dropout(pooled_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#     linear_output = self.linear(dropout_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to specify either input_ids or inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}]},{"cell_type":"code","source":[],"metadata":{"id":"bi0crHDL9tG7"},"execution_count":null,"outputs":[]}]}